{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Insights in Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be performing a syntax parsing analysis on a novel. The goal is to gain insight into the meaning of the text, the main topics of discussion, and the author's writing style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the NLTK library to tokenize the text into sentences, then tokenize those sentences into words. This results in a list of word tokenized sentences. \n",
    "\n",
    "From NLTK, we have imported word_tokenize and PunktSentenceTokenizer. We are using PunktSentenceTokenizer instead of sent_tokenizer because sent_tokenizer comes pre-trained and our text might be very different from that training text. For PunktSentenceTokenizer, the tokenizer is trained first on a training text and then, is deployed for tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def wordSentenceTokenize(text):\n",
    "    # create the tokenizer\n",
    "    sentenceTokenizer = PunktSentenceTokenizer(text)\n",
    "\n",
    "    # sentence tokenize the text\n",
    "    sentenceTokenized = sentenceTokenizer.tokenize(text)\n",
    "\n",
    "    # a list to hold our word tokenized sentences\n",
    "    wordTokenized = []\n",
    "    \n",
    "    # for every sentence, word tokenize it and add it to the list\n",
    "    for sentence in sentenceTokenized:\n",
    "        wordTokenized.append(word_tokenize(sentence))\n",
    "    \n",
    "    return wordTokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using regular expressions, we can define patterns of parts-of-speech tags and find chunks of words in the sentences who's tags match those patterns. This can help give insight into the meaning of a text. \n",
    "\n",
    "The helper functions below find the 30 most common noun phrase and verb phrase chunks in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def npChunkCounter(chunkedSentences):\n",
    "    # a list to hold chunks\n",
    "    chunks = []\n",
    "\n",
    "    # for every chunked sentence, extract the noun phrase chunks and add it to the list\n",
    "    for chunkedSentence in chunkedSentences:\n",
    "        # NP is a user defined label to represent 'noun phrase' chunks\n",
    "        for subtree in chunkedSentence.subtrees(filter=lambda t: t.label() == 'NP'): \n",
    "            chunks.append(tuple(subtree))\n",
    "    \n",
    "    # create a Counter object\n",
    "    chunkCounter = Counter()\n",
    "\n",
    "    # for every chunk in chunks, increase the counter of the specific chunk by 1 (works like a dict)\n",
    "    for chunk in chunks:\n",
    "        chunkCounter[chunk] += 1\n",
    "\n",
    "    # return the 30 most frequent noun phrase chunks\n",
    "    return chunkCounter.most_common(30)\n",
    "\n",
    "def vpChunkCounter(chunkedSentences):\n",
    "    # a list to hold chunks\n",
    "    chunks = []\n",
    "\n",
    "    # for every chunked sentence, extract the noun phrase chunks and add it to the list\n",
    "    for chunkedSentence in chunkedSentences:\n",
    "        # VP is a user defined label to represent 'noun phrase' chunks\n",
    "        for subtree in chunkedSentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "    \n",
    "    # create a Counter object\n",
    "    chunkCounter = Counter()\n",
    "\n",
    "    # for every chunk in chunks, increase the counter of the specific chunk by 1 (works like a dict)\n",
    "    for chunk in chunks:\n",
    "        chunkCounter[chunk] += 1\n",
    "\n",
    "    # return the 30 most frequent verb phrase chunks\n",
    "    return chunkCounter.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# Import text from file\n",
    "textLoc = './dorian_gray.txt'\n",
    "text = open(textLoc,encoding='utf-8').read().lower()\n",
    "\n",
    "# Tokenize the text into a list of word tokenized sentences\n",
    "wordTokenizedText = wordSentenceTokenize(text)\n",
    "\n",
    "# Sanity Check for word sentence tokenization\n",
    "testTokenizedSentence = wordTokenizedText[20]\n",
    "# print(testTokenizedSentence)\n",
    "\n",
    "# Create a list to host part-of-speech tagged sentences\n",
    "posTaggedText = []\n",
    "\n",
    "# for every word tokenized sentence, pos tag it and add that to the list\n",
    "for tokenizedSentence in wordTokenizedText:\n",
    "    posTaggedText.append(pos_tag(tokenizedSentence))\n",
    "\n",
    "# Sanity Check for pos tagging\n",
    "testPosSentence = posTaggedText[20]\n",
    "# print(testPosSentence)\n",
    "\n",
    "# define noun phrase and verb phrase chunk grammar \n",
    "npChunkGrammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "vpChunkGrammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create np and vp RegexpParser objects\n",
    "npChunkParser = RegexpParser(npChunkGrammar)\n",
    "vpChunkParser = RegexpParser(vpChunkGrammar)\n",
    "\n",
    "# create a list to hold the np and vp chunked sentences\n",
    "npChunkedText = []\n",
    "vpChunkedText = []\n",
    "\n",
    "# for every pos-tagged sentence, chunk the sentence and add them to the appropriate list\n",
    "for posSentence in posTaggedText:\n",
    "    npChunkedText.append(npChunkParser.parse(posSentence))\n",
    "    vpChunkedText.append(vpChunkParser.parse(posSentence))\n",
    "\n",
    "# Store and print the 30 most common np and vp chunks\n",
    "mostCommonNPChunks = npChunkCounter(npChunkedText)\n",
    "mostCommonVPChunks = vpChunkCounter(vpChunkedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ((('i', 'NN'),), 963),\n",
      "    ((('henry', 'NN'),), 200),\n",
      "    ((('lord', 'NN'),), 197),\n",
      "    ((('life', 'NN'),), 170),\n",
      "    ((('harry', 'NN'),), 136),\n",
      "    ((('dorian', 'JJ'), ('gray', 'NN')), 127),\n",
      "    ((('something', 'NN'),), 126),\n",
      "    ((('nothing', 'NN'),), 93),\n",
      "    ((('basil', 'NN'),), 85),\n",
      "    ((('the', 'DT'), ('world', 'NN')), 70),\n",
      "    ((('everything', 'NN'),), 69),\n",
      "    ((('anything', 'NN'),), 68),\n",
      "    ((('hallward', 'NN'),), 68),\n",
      "    ((('the', 'DT'), ('man', 'NN')), 61),\n",
      "    ((('the', 'DT'), ('room', 'NN')), 60),\n",
      "    ((('face', 'NN'),), 57),\n",
      "    ((('the', 'DT'), ('door', 'NN')), 56),\n",
      "    ((('love', 'NN'),), 55),\n",
      "    ((('art', 'NN'),), 52),\n",
      "    ((('course', 'NN'),), 51),\n",
      "    ((('the', 'DT'), ('picture', 'NN')), 46),\n",
      "    ((('the', 'DT'), ('lad', 'NN')), 45),\n",
      "    ((('head', 'NN'),), 44),\n",
      "    ((('round', 'NN'),), 44),\n",
      "    ((('hand', 'NN'),), 44),\n",
      "    ((('sibyl', 'NN'),), 41),\n",
      "    ((('the', 'DT'), ('table', 'NN')), 40),\n",
      "    ((('the', 'DT'), ('painter', 'NN')), 38),\n",
      "    ((('sir', 'NN'),), 38),\n",
      "    ((('a', 'DT'), ('moment', 'NN')), 38)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(mostCommonNPChunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at most_common_np_chunks, we can identify characters of importance such as henry, harry, dorian gray, and basil. They show up in the text most frequently!\n",
    "\n",
    "Moreover, another noun phrase 'the picture' appears to be very relevant and also 'the painter'. Possibly, this is a story about an artist? I don't know, I've never read the text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ((('i', 'NN'), ('am', 'VBP')), 101),\n",
      "    ((('i', 'NN'), ('was', 'VBD')), 40),\n",
      "    ((('i', 'NN'), ('want', 'VBP')), 37),\n",
      "    ((('i', 'NN'), ('know', 'VBP')), 33),\n",
      "    ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 32),\n",
      "    ((('i', 'NN'), ('have', 'VBP')), 32),\n",
      "    ((('i', 'NN'), ('had', 'VBD')), 31),\n",
      "    ((('i', 'NN'), ('suppose', 'VBP')), 17),\n",
      "    ((('i', 'NN'), ('think', 'VBP')), 16),\n",
      "    ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14),\n",
      "    ((('i', 'NN'), ('thought', 'VBD')), 13),\n",
      "    ((('i', 'NN'), ('believe', 'VBP')), 12),\n",
      "    ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11),\n",
      "    ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11),\n",
      "    ((('henry', 'NN'), ('had', 'VBD')), 11),\n",
      "    ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9),\n",
      "    ((('i', 'NN'), ('met', 'VBD')), 9),\n",
      "    ((('i', 'NN'), ('said', 'VBD')), 9),\n",
      "    ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8),\n",
      "    ((('i', 'NN'), ('see', 'VBP')), 8),\n",
      "    ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7),\n",
      "    ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7),\n",
      "    ((('life', 'NN'), ('has', 'VBZ')), 7),\n",
      "    ((('i', 'NN'), ('did', 'VBD')), 6),\n",
      "    ((('i', 'NN'), ('feel', 'VBP')), 6),\n",
      "    ((('life', 'NN'), ('is', 'VBZ')), 6),\n",
      "    ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6),\n",
      "    ((('i', 'NN'), ('asked', 'VBD')), 6),\n",
      "    ((('i', 'NN'), ('came', 'VBD')), 6),\n",
      "    ((('i', 'NN'), ('felt', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(mostCommonVPChunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the most_common_vp_chunks, we find something interesting about the theme of the text. The verb phrases 'i want', 'i know' and 'i have' occur frequently, representing a theme of desire and need."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddee34380c017b9a4435766bf49bcbd068cf54965e227858e99aeddcf59dc6ac"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
