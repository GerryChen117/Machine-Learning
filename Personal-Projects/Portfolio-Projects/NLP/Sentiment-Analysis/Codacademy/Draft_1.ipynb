{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling and Sentiment Analysis of Text Message Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Message</th>\n",
       "      <th>length</th>\n",
       "      <th>country</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10120</td>\n",
       "      <td>Bugis oso near wat...</td>\n",
       "      <td>21</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10121</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>111</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10122</td>\n",
       "      <td>I dunno until when... Lets go learn pilates...</td>\n",
       "      <td>46</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10123</td>\n",
       "      <td>Den only weekdays got special price... Haiz... Cant eat liao... Cut nails oso muz wait until i finish drivin wat, lunch still muz eat wat...</td>\n",
       "      <td>140</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10124</td>\n",
       "      <td>Meet after lunch la...</td>\n",
       "      <td>22</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>10125</td>\n",
       "      <td>m walking in citylink now ü faster come down... Me very hungry...</td>\n",
       "      <td>65</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10126</td>\n",
       "      <td>5 nights...We nt staying at port step liao...Too ex</td>\n",
       "      <td>51</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>10127</td>\n",
       "      <td>Hey pple...$700 or $900 for 5 nights...Excellent location wif breakfast hamper!!!</td>\n",
       "      <td>81</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10128</td>\n",
       "      <td>Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn</td>\n",
       "      <td>160</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>10129</td>\n",
       "      <td>Hey tmr maybe can meet you at yck</td>\n",
       "      <td>33</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10130</td>\n",
       "      <td>Oh...i asked for fun. Haha...take care. ü</td>\n",
       "      <td>41</td>\n",
       "      <td>SG</td>\n",
       "      <td>2003/4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the Dataset\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "originalData = pd.read_csv('./clean_nus_sms.csv')\n",
    "display(HTML(originalData[0:11].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the dataset doesn't come pre-labelled with sentiment. We might need to use TextBlob to label the data. The ultimate goal of this project is to conduct sentiment analysis on the text messages and find the most common topics users text about. Perhaps it would be interesting to perform this comparison by date and see how trends change over time, or, perform comparisons by country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My plans for preprocessing are:\n",
    "- Use TextBlob Python Library to perform spelling correction.\n",
    "- Noise Removal: Use Regex to remove punctuations/accents, special characters, numeric digits, [leading, ending, and vertical] whitespace, and HTML formatting. \n",
    "- Tokenization: Break the text messages into smaller components (text -> sentence level -> word level).\n",
    "- Normalization: Convert all text to lowercase, remove stopwords, apply stemming and lemmatization (powered by POS-tagging). \n",
    "\n",
    "- Is there a way to remove grammar contractions (i.e. don’t → do not)? And to convert emojis to the appropriate meaning of their occurence in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yun ah the ubi one say if ü wan call by tomorrow call 67441233 look for irene ere only got bus8 22 65 61 66 382 ubi cres ubi tech park 6ph for 1st 5wkg days èn \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"  Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\t \"\n",
    "cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a6d6db9694305e110445908428f78851d0d4f681eb81b5f3cc121d452b2ba86"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
